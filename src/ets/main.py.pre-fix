import os
import sys
import argparse
import logging
import time
import csv
from datetime import datetime
import pandas as pd
from ets.config.defaults import DEFAULT_WEIGHTS
DEFAULT_CFG = {'app': {'out_dir':'out','logs_dir':'logs','cache_dir':'cache'}}
from ets.util.dicts import deep_merge

def deep_get(d, path, default=None):
    cur = d if isinstance(d, dict) else {}
    for k in path:
        if not isinstance(cur, dict) or k not in cur:
            return default
        cur = cur[k]
    return cur

def ensure_dir_safe(path):
    import os
    if path:
        try: os.makedirs(path, exist_ok=True)
        except Exception: pass

# Quiet noisy libs
logging.getLogger("yfinance").setLevel(logging.ERROR)
logging.getLogger("urllib3").setLevel(logging.ERROR)

# --- Project imports
from ets.core.utils import load_yaml, run_id, ensure_dirs
from ets.data.calendar import from_finnhub, from_csv
from ets.core.factors import compute_raw_factors
from ets.core.normalization import robust_normalize_df
from ets.core.scoring import compute_scores
from ets.core.selection import apply_filters_and_select
from ets.outputs.csv_writer import (
    write_factors,
    write_scores,
    write_trades,
    write_pulls,
)
from ets.outputs.telemetry import write_telemetry
from ets.core.env import load_env, require_env
from ets.data.signals.calendar_loader import set_fallback_peers
from ets.core.finalize import finalize_results

# Finnhub-first provider registry + pull log
from ets.data.providers.provider_registry import ProviderRegistry
from ets.data.providers.quotes_agg import set_registry, get_pull_log
from ets.data.signals.calendar_loader import set_registry as set_calendar_registry

# ---------- Helpers ----------

def _config_paths():
    """Resolve config files relative to this module so the CLI works anywhere."""
    here = os.path.dirname(__file__)  # .../src/ets
    cfg_dir = os.path.join(here, "config")
    return (
        os.path.join(cfg_dir, "config.yaml"),
        os.path.join(cfg_dir, "weights.yaml"),
    )

def parse_args():
    ap = argparse.ArgumentParser(description="ETS: Pre-earnings MVP (free-tier)")
    ap.add_argument(
        "--date",
        default=datetime.now().strftime("%Y-%m-%d"),
        help="Run date YYYY-MM-DD (default: today)",
    )
    ap.add_argument(
        "--session",
        default="amc",
        choices=["amc", "bmo"],
        help="Earnings session: amc (after market close) or bmo (before market open)",
    )
    ap.add_argument(
        "--tickers",
        help="Path to CSV with tickers (one per line). If omitted, tries Finnhub calendar.",
    )
    return ap.parse_args()

def load_configs():
    cfg_path, wts_path = _config_paths()
    cfg = load_yaml(cfg_path)
cfg = deep_merge(DEFAULT_CFG, cfg or {})
    wts = DEFAULT_WEIGHTS
    if not isinstance(wts.get('caps'), dict):
        raise SystemExit("[FATAL] Weights missing 'caps' for session=%r." % (session,))
    ensure_dirs(cfg.get("app",{}).get("out_dir"), cfg.get("app",{}).get("logs_dir"), cfg.get("app",{}).get("cache_dir"))
    return cfg, wts

def _load_sector_map_from_cache(cache_dir: str = "cache") -> dict:
    """
    Read cache/sectors.csv if present. Format: TICKER,SECTOR (no header required).
    Returns dict like {'AAPL': 'Information Technology', ...}
    """
    path = os.path.join(cache_dir, "sectors.csv")
    mapping = {}
    if os.path.exists(path):
        try:
            with open(path, "r", newline="", encoding="utf-8") as f:
                rdr = csv.reader(f)
                for row in rdr:
                    if not row:
                        continue
                    t = str(row[0]).strip().upper()
                    s = (row[1] if len(row) > 1 else "Unknown").strip()
                    if t:
                        mapping[t] = s
        except Exception:
            pass
    return mapping

def build_universe(date_str: str, tickers_csv: str | None, provider_reg: ProviderRegistry | None):
    if tickers_csv:
        syms = from_csv(tickers_csv)
    else:
        # Use Finnhub if available; otherwise empty
        syms = from_finnhub(date_str)
    return syms or []

# ---------- Main ----------

def main():
    args = parse_args()
    cfg, wts = load_configs()

# Optional safe session override
    try:
        _sess = getattr(args, "session", None)
        _by = (wts.get("by_session", {}) if isinstance(wts, dict) else {})
        if _sess in _by: wts = deep_merge(wts, _by[_sess])
    except Exception:
        pass
    # Initialize providers (Finnhub-first) and attach to quotes aggregator
    load_env()
    # If you want to run without Finnhub calendar, comment the next line.
    try:
        require_env('FINNHUB_API_KEY', 'FINNHUB_API_KEY missing. Put it in .env')
    except Exception as e:
        print('[WARN]', e)
    reg = ProviderRegistry(cfg)
    set_registry(reg)
    set_calendar_registry(reg)

    rid = run_id(args.date, args.session)
    out_dir = cfg["app"]["out_dir"]

    # Sector map: prefer cache/sectors.csv; do NOT require config keys
    sector_map = _load_sector_map_from_cache(cfg["app"]["cache_dir"])

    # 1) Universe
    universe = build_universe(args.date, args.tickers, reg)
    set_fallback_peers(list(universe))
    if not universe:
        print("No tickers found from calendar or CSV. Provide --tickers path.")
        sys.exit(0)

    # 2) Raw factors (gentle throttle to reduce upstream 429s even with limiter)
    raw_rows = []
    for idx, t in enumerate(universe):
        row = compute_raw_factors(t, sector_map)
        if row:
            raw_rows.append(row)
        # very light sleep to avoid synchronized bursts across providers
        if idx % 3 == 0:
            time.sleep(0.20)

    if not raw_rows:
        print("No factor rows computed.")
        sys.exit(0)

    factors_df = pd.DataFrame(raw_rows)
    # Defensive aliases to smooth legacy/new column naming
    if "price" not in factors_df.columns and "last" in factors_df.columns:
        factors_df["price"] = factors_df["last"]
    if "dollar_vol" not in factors_df.columns:
        if "dollar_volume" in factors_df.columns:
            factors_df["dollar_vol"] = factors_df["dollar_volume"]
        elif {"last","volume"}.issubset(factors_df.columns):
            factors_df["dollar_vol"] = factors_df["last"] * factors_df["volume"]
        else:
            factors_df["dollar_vol"] = 0.0

    # ensure selection keys exist (aliases for older selection.py)
    if "last" in factors_df.columns and "price" not in factors_df.columns:
        factors_df["price"] = factors_df["last"]
    if "dollar_vol" not in factors_df.columns:
        if "dollar_volume" in factors_df.columns:
            factors_df["dollar_vol"] = factors_df["dollar_volume"]
        elif {"last","volume"}.issubset(factors_df.columns):
            factors_df["dollar_vol"] = factors_df["last"] * factors_df["volume"]
        else:
            factors_df["dollar_vol"] = 0.0
    # ensure selection keys exist
    if "last" in factors_df.columns and "price" not in factors_df.columns:
        factors_df["price"] = factors_df["last"]

    # 3) Normalize
    norm_cols = ["M_raw", "V_raw", "S_raw", "A_raw", "sigma_raw", "tau_raw", "CAL_raw", "SRM_raw", "PEER_raw", "ETFF_raw", "VIX_raw", "TREND_raw"]
    factors_df = robust_normalize_df(
        factors_df,
        cols=norm_cols,
        p_low=cfg["normalization"]["winsor_p_low"],
        p_high=cfg["normalization"]["winsor_p_high"],
        min_universe_for_robust=cfg["normalization"]["min_universe_for_robust"],
        clip_min=cfg["normalization"]["clip_min"],
        clip_max=cfg["normalization"]["clip_max"],
    )
    # rename *_raw_norm -> *_norm
    rename_map = {c + "_norm": c.replace("_raw", "") + "_norm" for c in norm_cols}
    factors_df.rename(columns=rename_map, inplace=True)

    # 4) Score
    df_scores = compute_scores(
        factors_df,
        base=wts["base"],
        mult=wts["multipliers"],
        caps=wts["caps"],
    )

    # 5) Select
    sel_cfg = cfg["selection"]
    uni_cfg = cfg["universe"]
    df_trades = apply_filters_and_select(
        df_scores,
        min_price=uni_cfg["min_price"],
        dollar_volume_floor=uni_cfg["dollar_volume_floor"],
        risk_floor_sigma_g=sel_cfg["risk_floor_sigma_g"],
        risk_floor_tau_g=sel_cfg["risk_floor_tau_g"],
        score_threshold=sel_cfg["score_threshold"],
        max_names=sel_cfg["max_names"],
        max_per_sector=sel_cfg["max_per_sector"],
    )

    # 6) Write outputs (including pull & provider telemetry)
    factors_path = os.path.join(out_dir, f"{rid}_factors.csv")
    scores_path  = os.path.join(out_dir, f"{rid}_scores.csv")
    trades_path  = os.path.join(out_dir, f"{rid}_trades.csv")
    pulls_path   = os.path.join(out_dir, f"{rid}_pulls.csv")
    tele_path    = os.path.join(out_dir, f"{rid}_telemetry.csv")

    write_factors(factors_path, raw_rows)
    df_scores.to_csv(scores_path, index=False)
    df_trades.to_csv(trades_path, index=False)
    from ets.data.providers.quotes_agg import get_pull_log
    write_pulls(pulls_path, get_pull_log())
    write_telemetry(tele_path, reg.stats())

    # write trader-friendly clean file
    finalize_results(scores_path, out_dir)

    print(f"[OK] Wrote:\n  {factors_path}\n  {scores_path}\n  {trades_path}\n  {pulls_path}\n  {tele_path}")

if __name__ == "__main__":
    main()
